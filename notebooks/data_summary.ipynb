{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72867c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T15:18:27.809990Z",
     "start_time": "2024-10-21T15:18:27.806893Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from torch.utils.data import DataLoader, Dataset, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e87a2bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T15:18:29.514323Z",
     "start_time": "2024-10-21T15:18:29.511724Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "currently available samples:\n",
    "SQ1631_s1_R, SQ1631_s2_R, SQ1631_s3_N, SQ1631_s4_N\n",
    "SQ1632_s1_R, SQ1632_s2_N, SQ1632_s3_R, SQ1632_s4_N\n",
    "SQ1633_s1_R, SQ1633_s2_R, SQ1633_s3_N, SQ1633_s4_N\n",
    "'''\n",
    "sample_ls = ['SQ1631_s1_R', 'SQ1631_s2_R', 'SQ1631_s3_N', 'SQ1631_s4_N',\n",
    "             'SQ1632_s1_R', 'SQ1632_s2_N', 'SQ1632_s3_R', 'SQ1632_s4_N',\n",
    "             'SQ1633_s1_R', 'SQ1633_s2_R', 'SQ1633_s3_N', 'SQ1633_s4_N']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9287377-760e-4130-9ee6-6a009fadbe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all channel means\n",
    "channel_names = []\n",
    "channel_means = []\n",
    "for sample_name in sample_ls:\n",
    "    slide = np.load(f'../data/npy/{sample_name}.npy', mmap_mode='r')\n",
    "    mask = np.sum(slide, axis=0) > 0\n",
    "    nonzero = slide[:, mask]\n",
    "    channel_mean = np.mean(nonzero, axis=1)\n",
    "    channels = pd.read_csv(f'../data/npy/{sample_name}_channel_names.csv', header=None, index_col=0)\n",
    "    channels = [name[0] for name in channels.to_numpy()]\n",
    "    channels = [mz.split('-±')[0] for mz in channels]\n",
    "    channel_means.append(pd.DataFrame(channel_mean, index=channels, columns=[sample_name]))\n",
    "    channel_names.append(channels)\n",
    "channel_means = pd.concat(channel_means, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cf086a-dfd3-4125-9d7e-fae7a795b09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all channels are in the same order\n",
    "def all_arrays_identical(array_list):\n",
    "    if not array_list:\n",
    "        return True  # An empty list is considered to have all identical elements\n",
    "    first_array = array_list[0]\n",
    "    return all(np.array_equal(first_array, arr) for arr in array_list)\n",
    "\n",
    "all_arrays_identical(channel_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80849c6-65f2-436b-a911-67ae9af028ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_means.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d419ec-816f-4a75-a021-09646ca8408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(channel_means.T, cmap=\"Reds\", vmin=0, vmax=255, figsize=(10, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850c56b3-639e-4117-8247-e99f54e68b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try random forest classifier on channel_means data\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525a9f8e-87aa-496d-ad46-bc481f896084",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [name[-1] for name in channel_means.columns]\n",
    "print(labels)\n",
    "response = [1 if lab == 'R' else 0 for lab in labels]\n",
    "print(response)\n",
    "\n",
    "X = channel_means.T.to_numpy()\n",
    "y = np.asarray(response)\n",
    "\n",
    "# Create a RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "# Create a KFold object with 4 splits\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(rf, X, y, cv=kf)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores for each fold: \", cv_scores)\n",
    "print(\"Mean cross-validation score: \", np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0213c5-8fe2-44ff-a403-f3084156d4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store predictions and true labels\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "# Perform manual cross-validation\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split the data into training and validation sets\n",
    "    print(train_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit the model on the training set\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_pred = rf.predict(X_test)\n",
    "    \n",
    "    # Store predictions and true labels\n",
    "    all_predictions.append(y_pred)\n",
    "    all_true_labels.append(y_test)\n",
    "    \n",
    "    # Print predictions and true labels for the current fold\n",
    "    print(f\"Predictions for fold: {y_pred}\")\n",
    "    print(f\"True labels for fold: {y_test}\")\n",
    "    print(\"----------\")\n",
    "\n",
    "# Convert the lists of arrays to a single array\n",
    "all_predictions = np.concatenate(all_predictions)\n",
    "all_true_labels = np.concatenate(all_true_labels)\n",
    "\n",
    "# Print overall predictions and true labels\n",
    "print(\"All predictions:\", all_predictions)\n",
    "print(\"All true labels:\", all_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a136c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect individual samples\n",
    "sample_name = sample_ls[0]\n",
    "slide = np.load(f'../data/npy/{sample_name}.npy', mmap_mode='r')\n",
    "channels = pd.read_csv(f'../data/npy/{sample_name}_channel_names.csv', header=None, index_col=0)\n",
    "channels = [name[0] for name in channels.to_numpy()]\n",
    "print(channels[0:10])\n",
    "channels = [mz.split('-±')[0] for mz in channels]\n",
    "print(channels[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(slide.shape)\n",
    "mask = np.sum(slide, axis=0) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e9aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757f3aa6-a6a0-42e5-b89f-fe3e0bbb8327",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(slide[750, ], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d226711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel wise correlation\n",
    "nonzero = slide[:, mask]\n",
    "print(f'None zero pixels: {nonzero.shape}')\n",
    "corr = np.corrcoef(nonzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fc5c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(pd.DataFrame(corr, index=channels, columns=channels), cmap=\"RdBu_r\", vmin=-1, vmax=1, figsize=(6, 6))\n",
    "plt.show()\n",
    "#plt.imshow(corr, cmap='RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675da715-3cf8-4d74-806f-c819630c5656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tiles\n",
    "def tile_array(array, tile_size, overlap, threshold):\n",
    "    c, h, w = array.shape\n",
    "    step_size = tile_size - overlap\n",
    "    nx = w // step_size + 1\n",
    "    ny = h // step_size + 1\n",
    "    mask = np.sum(array, axis=0) > 0\n",
    "    tiles = []\n",
    "    coords = []\n",
    "    for i in range(nx):\n",
    "        for j in range(ny):\n",
    "            \n",
    "            x = i * step_size\n",
    "            y = j * step_size\n",
    "            x_end = min((x + tile_size), w)\n",
    "            y_end = min((y + tile_size), h)\n",
    "            if np.mean(mask[y :y_end, x : x_end]) > threshold:\n",
    "                tile = np.zeros((c, tile_size, tile_size))\n",
    "                tile[:,: y_end-y, :x_end-x] = array[:, y :y_end, x : x_end]\n",
    "                tiles.append(tile)\n",
    "                coords.append([x, y])\n",
    "            else:\n",
    "                pass\n",
    "    return tiles, coords\n",
    "\n",
    "class TileDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        from torchvision.transforms import v2\n",
    "        self.transform = v2.Compose([\n",
    "                                        #v2.ToImage(),\n",
    "                                        v2.ToDtype(torch.float, scale=True),\n",
    "                                        #v2.Lambda(lambda x: (x - 0.5)*2)\n",
    "                                        ])\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #x = np.expand_dims(self.inputs[index], 0)\n",
    "        x = torch.Tensor(self.inputs[index])\n",
    "        x = self.transform(x)\n",
    "        #print(x.shape)\n",
    "        y = torch.tensor(self.labels[index], dtype=torch.float)\n",
    "        return x, y\n",
    "\n",
    "    \n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(nn.Conv2d(in_channels, 16, 1),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Conv2d(16, 8, 3, stride=2),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Conv2d(8, 8, 3, stride=2),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Conv2d(8, 8, 3, stride=2),\n",
    "                                   nn.ReLU()\n",
    "                                  )\n",
    "        self.fc = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        #print(x.shape)\n",
    "        x = torch.mean(x, dim = (2, 3), keepdim = False) # Global pooling\n",
    "        #print(x.shape)\n",
    "        out = self.fc(x)\n",
    "        out = torch.squeeze(out)\n",
    "        if out.ndim == 0:\n",
    "            out = out.unsqueeze(0)\n",
    "        return out\n",
    "    \n",
    "def train_and_evaluate(n_epochs, model, trn_loader, tst_loader):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    accuracy_fun = torchmetrics.Accuracy(task='binary')\n",
    "    roc_fun = torchmetrics.AUROC(task='binary')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(n_epochs):\n",
    "        print('EPOCH {}:'.format(epoch + 1))\n",
    "        model.train()\n",
    "        running_loss = 0.\n",
    "        for i, (inputs, labels) in enumerate(trn_loader):\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9:  # Print every 100 batches\n",
    "                print(f'Epoch [{epoch + 1}/{n_epochs}], Step [{i + 1}/{len(trn_loader)}], Loss: {running_loss / 100:.4f}')\n",
    "                running_loss = 0.0\n",
    "    print('Finished Training')\n",
    "\n",
    "    # test model and report metric\n",
    "    model.eval()\n",
    "    running_loss = 0.\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(tst_loader):\n",
    "            logits = model(inputs)\n",
    "            all_labels.append(labels)\n",
    "            all_probs.append(nn.Sigmoid()(logits).cpu())\n",
    "            loss = criterion(logits, labels)\n",
    "            running_loss += loss\n",
    "    all_probs = torch.cat(all_probs)\n",
    "    print(all_probs)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    print(all_labels)\n",
    "    avg_loss = running_loss / (i + 1)\n",
    "    accuracy = accuracy_fun(all_probs, all_labels)\n",
    "    auc = roc_fun(all_probs, all_labels)\n",
    "    print(f'Test loss: {avg_loss}; AUROC: {auc}; Accuracy: {accuracy}')\n",
    "    return accuracy, auc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5279275-c535-4d63-b54e-a4b552fdbfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_ls = []\n",
    "label_ls = []\n",
    "tile_grp = []\n",
    "tile_coords = []\n",
    "\n",
    "for sample_name in sample_ls[0:4]:\n",
    "    slide = np.load(f'../data/npy/{sample_name}.npy', mmap_mode='r')\n",
    "    tiles, coords = tile_array(slide, tile_size=32, overlap=8, threshold=0.2)\n",
    "    labels = [1 if sample_name[-1] == 'R' else 0] * len(tiles)\n",
    "    print(f'{len(tiles)} tiles generated in sample {sample_name}.')\n",
    "    tile_ls.extend(tiles)\n",
    "    label_ls.extend(labels)\n",
    "    tile_grp.extend([sample_name] * len(tiles))\n",
    "    tile_coords.extend(coords)\n",
    "\n",
    "tile_ls = np.asarray(tile_ls)\n",
    "label_ls = np.asarray(label_ls)\n",
    "print(tile_ls.shape)\n",
    "print(label_ls.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d8e5e7-b004-404d-9a04-3991c9c8ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_epochs = 10 \n",
    "print(tile_ls[0].shape)\n",
    "tile_dat = TileDataset(inputs=tile_ls, labels=label_ls)\n",
    "print(len(tile_dat))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, labels = zip(*batch)\n",
    "    #print(inputs[0].shape)\n",
    "    inputs = torch.stack(inputs, 0)\n",
    "    labels = torch.stack(labels)\n",
    "    #print(inputs.shape)\n",
    "    #print(labels)\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    return inputs, labels\n",
    "        \n",
    "sgkf = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "for i, (trn_idx, tst_idx) in enumerate(sgkf.split(X=np.zeros(len(label_ls)), y=label_ls, groups=tile_grp)):\n",
    "    print(f'Fold {i}')\n",
    "    print('--------------------------------')\n",
    "    trn_subset = Subset(tile_dat, trn_idx)\n",
    "    tst_subset = Subset(tile_dat, tst_idx)\n",
    "    print(trn_idx)\n",
    "    print(tst_idx)\n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trn_loader = DataLoader(trn_subset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "    tst_loader = DataLoader(tst_subset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialize the model for this fold\n",
    "    model = SimpleCNN(in_channels=917).to(device)\n",
    "    auc, accuracy = train_and_evaluate(n_epochs, model, trn_loader, tst_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3f8582-6c84-4a19-9cf6-3fa5f304848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(tile_grp)[tst_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880cfaa9-4da9-4bbe-ab47-df9c8c985591",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tile_ls[0][750,:,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae53ea1-ef99-424d-8222-22dffa7b0026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0632849-003c-44e4-b76f-1cf4aca90ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
